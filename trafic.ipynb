{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pip install opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.5.5.64)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from opencv-python) (1.21.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import cv\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
    "from numba import jit, cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First storing data for preprocesssing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "classes = 43\n",
    "cur_path = os.getcwd()\n",
    "\n",
    "#Retrieving the images and their labels \n",
    "for i in range(classes):\n",
    "    path = os.path.join(cur_path,'train',str(i))\n",
    "    images = os.listdir(path)\n",
    "    for a in images:\n",
    "        try:\n",
    "            image = Image.open(path + '\\\\'+ a)\n",
    "            image = image.resize((30,30))\n",
    "            image = np.array(image)\n",
    "            data.append(image)\n",
    "            labels.append(i)\n",
    "        except:\n",
    "            print(\"Error loading image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing import image\n",
    "\n",
    "# for j in range(43,45):\n",
    "#     path1 = os.path.join(cur_path,'train',str(j))\n",
    "#     directory = os.listdir(path1)\n",
    "#     for i in directory:\n",
    "#         try:\n",
    "#             test_image = image.load_img(f\"{path1}\\{i}\", target_size = (30, 30))\n",
    "#             test_image = image.img_to_array(test_image)\n",
    "#             data.append(list(test_image))\n",
    "#             labels.append(j)\n",
    "#         except:\n",
    "#             print(\"error\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39209, 30, 30, 3) (39209,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Converting lists into numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# # print(data)\n",
    "print(data.shape, labels.shape)\n",
    "#Splitting training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# #Converting the labels into one hot encoding\n",
    "y_train = to_categorical(y_train, 43)\n",
    "y_test = to_categorical(y_test, 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(43, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Compilation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "491/491 [==============================] - 58s 116ms/step - loss: 1.2574 - accuracy: 0.6879 - val_loss: 0.1362 - val_accuracy: 0.9716\n",
      "Epoch 2/20\n",
      "491/491 [==============================] - 60s 123ms/step - loss: 0.1822 - accuracy: 0.9538 - val_loss: 0.0571 - val_accuracy: 0.9862\n",
      "Epoch 3/20\n",
      "491/491 [==============================] - 61s 124ms/step - loss: 0.1081 - accuracy: 0.9723 - val_loss: 0.0487 - val_accuracy: 0.9893\n",
      "Epoch 4/20\n",
      "491/491 [==============================] - 62s 127ms/step - loss: 0.0897 - accuracy: 0.9767 - val_loss: 0.0306 - val_accuracy: 0.9923\n",
      "Epoch 5/20\n",
      "491/491 [==============================] - 63s 128ms/step - loss: 0.0714 - accuracy: 0.9828 - val_loss: 0.0278 - val_accuracy: 0.9940\n",
      "Epoch 6/20\n",
      "491/491 [==============================] - 71s 145ms/step - loss: 0.0596 - accuracy: 0.9857 - val_loss: 0.0315 - val_accuracy: 0.9922\n",
      "Epoch 7/20\n",
      "491/491 [==============================] - 68s 138ms/step - loss: 0.0736 - accuracy: 0.9816 - val_loss: 0.0400 - val_accuracy: 0.9917\n",
      "Epoch 8/20\n",
      "491/491 [==============================] - 68s 138ms/step - loss: 0.0545 - accuracy: 0.9865 - val_loss: 0.0353 - val_accuracy: 0.9927\n",
      "Epoch 9/20\n",
      "491/491 [==============================] - 65s 132ms/step - loss: 0.0562 - accuracy: 0.9857 - val_loss: 0.0281 - val_accuracy: 0.9939\n",
      "Epoch 10/20\n",
      "491/491 [==============================] - 66s 135ms/step - loss: 0.0444 - accuracy: 0.9879 - val_loss: 0.0414 - val_accuracy: 0.9904\n",
      "Epoch 11/20\n",
      "491/491 [==============================] - 71s 145ms/step - loss: 0.0540 - accuracy: 0.9860 - val_loss: 0.0229 - val_accuracy: 0.9954\n",
      "Epoch 12/20\n",
      "491/491 [==============================] - 75s 154ms/step - loss: 0.0703 - accuracy: 0.9828 - val_loss: 0.0324 - val_accuracy: 0.9934\n",
      "Epoch 13/20\n",
      "491/491 [==============================] - 77s 157ms/step - loss: 0.0436 - accuracy: 0.9901 - val_loss: 0.0319 - val_accuracy: 0.9921\n",
      "Epoch 14/20\n",
      "491/491 [==============================] - 82s 168ms/step - loss: 0.0421 - accuracy: 0.9897 - val_loss: 0.0400 - val_accuracy: 0.9938\n",
      "Epoch 15/20\n",
      "491/491 [==============================] - 91s 185ms/step - loss: 0.0491 - accuracy: 0.9879 - val_loss: 0.0258 - val_accuracy: 0.9938\n",
      "Epoch 16/20\n",
      "491/491 [==============================] - 77s 156ms/step - loss: 0.0467 - accuracy: 0.9887 - val_loss: 0.0492 - val_accuracy: 0.9931\n",
      "Epoch 17/20\n",
      "491/491 [==============================] - 74s 151ms/step - loss: 0.0427 - accuracy: 0.9897 - val_loss: 0.0306 - val_accuracy: 0.9957\n",
      "Epoch 18/20\n",
      "491/491 [==============================] - 72s 147ms/step - loss: 0.0406 - accuracy: 0.9902 - val_loss: 0.0435 - val_accuracy: 0.9921\n",
      "Epoch 19/20\n",
      "491/491 [==============================] - 73s 149ms/step - loss: 0.0529 - accuracy: 0.9879 - val_loss: 0.0287 - val_accuracy: 0.9946\n",
      "Epoch 20/20\n",
      "491/491 [==============================] - 73s 148ms/step - loss: 0.0535 - accuracy: 0.9886 - val_loss: 0.0310 - val_accuracy: 0.9943\n"
     ]
    }
   ],
   "source": [
    "# #Compilation of the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# @jit(target =\"cuda\")\n",
    "# epoch = 50\n",
    "#more accuracy increase the epochs size\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=20, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAving the Model for further Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Alert_classifier.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a23257344542c70b70a498512f46db94a3a7f44d371a6893c689768adca66338"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
